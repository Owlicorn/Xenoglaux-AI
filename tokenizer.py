import os
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace
from typing import List, Dict
import config

class XenoglauxTokenizer:
    def __init__(self):
        self.config = config.Config()
        self.tokenizer = None
        self.vocab_size = None
        
        # Special tokens
        self.special_tokens = [
            "[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]",
            "[EOS]", "[THINK]"
        ]
    
    def train_tokenizer(self, data_loader):
        """Train BPE tokenizer on combined data with special tokens"""
        print("üîÑ Training BPE tokenizer...")
        
        # Initialize tokenizer
        self.tokenizer = Tokenizer(BPE())
        self.tokenizer.pre_tokenizer = Whitespace()
        
        # Get training data for tokenizer
        training_pairs = data_loader.get_training_pairs(max_examples=5000)
        
        # Prepare text for tokenizer training
        texts = []
        for pair in training_pairs:
            texts.append(pair['input'])
            texts.append(pair['output'])
            if pair.get('thinking'):
                texts.append(pair['thinking'])
        
        # Use config VOCAB_SIZE
        vocab_size = self.config.VOCAB_SIZE
        
        print(f"üìù Using vocabulary size: {vocab_size}")
        
        trainer = BpeTrainer(
            vocab_size=vocab_size,
            special_tokens=self.special_tokens,
            min_frequency=2
        )
        
        # Train tokenizer
        self.tokenizer.train_from_iterator(texts, trainer=trainer)
        self.vocab_size = self.tokenizer.get_vocab_size()
        
        print(f"‚úÖ Tokenizer trained with {self.vocab_size} tokens")
    
    def encode(self, text: str) -> List[int]:
        """Encode text to token IDs"""
        if not self.tokenizer:
            raise ValueError("Tokenizer not trained")
        return self.tokenizer.encode(text).ids
    
    def decode(self, token_ids: List[int]) -> str:
        """Decode token IDs to text"""
        if not self.tokenizer:
            raise ValueError("Tokenizer not trained")
        return self.tokenizer.decode(token_ids)
    
    def get_special_tokens(self) -> Dict[str, int]:
        """Get special token mappings"""
        if not self.tokenizer:
            return {}
        return {token: self.tokenizer.token_to_id(token) for token in self.special_tokens}
    
    def save(self, path: str):
        """Save tokenizer - FIXED DIRECTORY CREATION"""
        if self.tokenizer:
            # Create directory if it doesn't exist
            os.makedirs(os.path.dirname(path), exist_ok=True)
            self.tokenizer.save(path)
            print(f"üíæ Tokenizer saved to {path}")
    
    def load(self, path: str):
        """Load tokenizer"""
        if os.path.exists(path):
            self.tokenizer = Tokenizer.from_file(path)
            self.vocab_size = self.tokenizer.get_vocab_size()
            print(f"üìÇ Tokenizer loaded from {path}")
        else:
            print(f"‚ùå Tokenizer file not found at {path}")
            