import os
import math
from typing import Dict, Any, List
from dotenv import load_dotenv

load_dotenv()

class Config:
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    LANG_MODEL_PATH = os.path.join(BASE_DIR, "data", "lang_model.txt")
    MONGODB_URL = os.getenv("MONGODB_URL")
    DATABASE_NAME = "xenoglaux_db"
    COLLECTION_NAME = "dataset"
    
    # New Math Training Data Source
    MATHS_TRAINING_URL = os.getenv("MATHS_TRAINING")
    MATHS_TRAINING_DB = "dataset"
    MATHS_TRAIN_COLLECTION = "NumCrunch"
    
    # Auto-scaling parameters (will be set automatically)
    D_MODEL = 512
    N_LAYERS = 8
    N_HEADS = 8
    D_FF = 2048
    VOCAB_SIZE = 50000
    
    # Fixed parameters
    DROPOUT = 0.1
    BATCH_SIZE = 8
    LEARNING_RATE = 3e-4
    EPOCHS = 15
    MAX_SEQUENCE_LENGTH = 512
    
    # Generation parameters
    MAX_GENERATION_LENGTH = 512
    MIN_GENERATION_LENGTH = 1
    TOP_K = 50
    TOP_P = 0.85
    TEMPERATURE = 0.7
    REPETITION_PENALTY = 1.5
    
    # Data loading limits - ENFORCE THESE STRICTLY
    MAX_MONGO_EXAMPLES = 100000
    MAX_MATHS_EXAMPLES = 30
    MAX_LANG_MODEL_LINES = 100000
    
    # Training monitoring
    SAMPLE_PROMPTS = ["Hello", "How are you?", "Who created you?", "What is mass of H?", "Solve 2+2", "Calculate derivative of x^2"]
    
    # Mood system removed - responses are mood-free
    
    # API Configuration
    API_HOST = "0.0.0.0"
    API_PORT = 8000
    DEBUG = True
    
    # Model paths
    MODEL_SAVE_PATH = "models/xenoglaux_model"
    TOKENIZER_SAVE_PATH = "models/tokenizer"
    
    @classmethod
    def auto_scale(cls, data_size: int, unique_tokens: int, total_estimated_data: int = None):
        """Auto-scale model based on FULL data size, not just sample"""
        # Use the larger of actual sample or estimated total
        effective_size = total_estimated_data if total_estimated_data and total_estimated_data > data_size else data_size
        
        print(f"ðŸ“Š Auto-scaling for {effective_size:,} total examples (sample: {data_size:,}), {unique_tokens:,} unique tokens")
        
        # AGGRESSIVE SCALING FOR LARGE DATASETS
        if effective_size >= 80000:  # Your actual data size ~100K
            cls.D_MODEL = 1024
            cls.N_LAYERS = 12
            cls.N_HEADS = 16
            cls.D_FF = 4096
            cls.VOCAB_SIZE = min(100000, unique_tokens + 10000)
            print("ðŸš€ LARGE MODEL: 100K+ examples detected")
            
        elif effective_size >= 50000:
            cls.D_MODEL = 768
            cls.N_LAYERS = 10
            cls.N_HEADS = 12
            cls.D_FF = 3072
            cls.VOCAB_SIZE = min(75000, unique_tokens + 5000)
            print("ðŸ“ˆ MEDIUM-LARGE MODEL: 50K+ examples detected")
            
        elif effective_size >= 20000:
            cls.D_MODEL = 512
            cls.N_LAYERS = 8
            cls.N_HEADS = 8
            cls.D_FF = 2048
            cls.VOCAB_SIZE = min(50000, unique_tokens + 3000)
            print("ðŸ“Š MEDIUM MODEL: 20K+ examples detected")
            
        elif effective_size >= 10000:
            cls.D_MODEL = 384
            cls.N_LAYERS = 6
            cls.N_HEADS = 6
            cls.D_FF = 1536
            cls.VOCAB_SIZE = min(40000, unique_tokens + 2000)
            print("ðŸ“š SMALL-MEDIUM MODEL: 10K+ examples detected")
            
        else:
            cls.D_MODEL = 256
            cls.N_LAYERS = 4
            cls.N_HEADS = 4
            cls.D_FF = 1024
            cls.VOCAB_SIZE = min(30000, unique_tokens + 1000)
            print("ðŸ”¬ SMALL MODEL: <10K examples")
        
        # Calculate estimated parameters
        embedding_params = cls.VOCAB_SIZE * cls.D_MODEL
        attention_params = cls.N_LAYERS * (4 * cls.D_MODEL * cls.D_MODEL)
        ff_params = cls.N_LAYERS * (2 * cls.D_MODEL * cls.D_FF)
        total_params = embedding_params + attention_params + ff_params
        estimated_mb = (total_params * 4) / (1024 * 1024)  # 4 bytes per parameter
        
        print(f"ðŸŽ¯ Final Model: d_model={cls.D_MODEL}, layers={cls.N_LAYERS}, heads={cls.N_HEADS}")
        print(f"ðŸ“ˆ Model size: {total_params:,} params, ~{estimated_mb:.1f}MB")
        print(f"ðŸ“š Vocabulary: {cls.VOCAB_SIZE}")
    
    @classmethod
    def to_dict(cls) -> Dict[str, Any]:
        return {k: v for k, v in cls.__dict__.items() if not k.startswith('_') and not callable(v)}